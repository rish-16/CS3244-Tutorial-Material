{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines â€“ An In-depth Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "05sNYyltOOMD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoCyq5f7OVwZ"
      },
      "source": [
        "class SupperVectorClassifier:\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.iterations = 1000\n",
        "\n",
        "    def hinge_loss(self, YHAT, Y):\n",
        "        '''\n",
        "        Hinge Loss from lecture. No changes made.\n",
        "        '''\n",
        "\n",
        "        distances = 1 - (Y * YHAT)\n",
        "        distances[distances < 0] = 0 # everywhere it's the correct prediction, give a loss of 0\n",
        "        return np.sum(distances) / len(YHAT) # average loss\n",
        "\n",
        "    def gradient_descent(self, X, Y, loss):\n",
        "        '''\n",
        "        Vanilla gradient descent. \n",
        "        \n",
        "        You can switch this to SGD as well to improve performance.\n",
        "        '''\n",
        "\n",
        "        grads = {}\n",
        "        loss = 1 - (Y * np.dot(X, self.w))\n",
        "        dw = np.zeros(len(self.w))\n",
        "        \n",
        "        for ind, d in enumerate(loss):\n",
        "            if max(0, d) == 0:\n",
        "                di = self.w\n",
        "            else:\n",
        "                di = self.w - (Y[ind] * X[ind])\n",
        "            dw += di\n",
        "        \n",
        "        dw = dw / len(Y)  # get the average gradient\n",
        "        grads['dw'] = dw\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads, alpha):\n",
        "        '''\n",
        "        Performs the actual update step in gradient descent.\n",
        "\n",
        "        grads : gradient of loss wrt weights\n",
        "        alpha : learning rate\n",
        "        '''\n",
        "        self.w = self.w - alpha * grads['dw']\n",
        "\n",
        "    def fit(self, X, Y, alpha=1e-2):\n",
        "        '''\n",
        "        Fits the model on the given dataset.\n",
        "\n",
        "        X: data samples\n",
        "        Y: binary labels (1 or 0)\n",
        "        alpha: step size / learning rate\n",
        "        '''\n",
        "\n",
        "        # reset the parameters for every call to fit\n",
        "        self.w = np.random.rand(X[0].shape[-1]) # get the number of features per sample\n",
        "\n",
        "        # perform the N iterations of learning\n",
        "        for i in range(self.iterations):\n",
        "            # forward pass\n",
        "            YHAT = np.dot(X, self.w)\n",
        "            loss = self.hinge_loss(YHAT, Y)\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                print (\"Iteration: {} | Loss: {}\".format(i, loss))\n",
        "\n",
        "            # backward pass\n",
        "            grads = self.gradient_descent(X, Y, loss) # calculate gradient wrt parameters\n",
        "            self.update(grads, alpha) # optimise the parameters\n",
        "    \n",
        "    def predict(self, X):\n",
        "        # simply compute forward pass\n",
        "        return np.dot(X, self.w)\n",
        "\n",
        "    def evaluate(self, X_test, Y_test):\n",
        "        '''\n",
        "        Returns the accuracy of the model.\n",
        "        '''\n",
        "        pred = self.predict(X_test)\n",
        "\n",
        "        # anything negative gets label -1, anything positive gets label 1\n",
        "        pred[pred < 0] = -1 \n",
        "        pred[pred >= 0] = 1\n",
        "        correct = 0\n",
        "\n",
        "        for i in range(len(Y_test)):\n",
        "            if pred[i] == Y_test[i]:\n",
        "                correct += 1\n",
        "\n",
        "        return correct / len(Y_test) # get final accuracy based on number of correct samples"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKilb4EqQq6K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, Y = datasets.load_breast_cancer(return_X_y=True)\n",
        "Y[Y == 0] = -1 # switch labels from [0, 1] to [-1, 1]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YywW366QQ6L8",
        "outputId": "720d4170-9a26-4c51-959f-7ab81b044bdf"
      },
      "source": [
        "model = SupperVectorClassifier()\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 | Loss: 416.2350522151881\n",
            "Iteration: 20 | Loss: 634.7471121710325\n",
            "Iteration: 40 | Loss: 1377.959569044146\n",
            "Iteration: 60 | Loss: 337.2871462020824\n",
            "Iteration: 80 | Loss: 174.50232848883326\n",
            "Iteration: 100 | Loss: 144.48051916551537\n",
            "Iteration: 120 | Loss: 144.97272350980242\n",
            "Iteration: 140 | Loss: 157.49100221912383\n",
            "Iteration: 160 | Loss: 188.68354119350255\n",
            "Iteration: 180 | Loss: 198.83279680794266\n",
            "Iteration: 200 | Loss: 201.37397024816923\n",
            "Iteration: 220 | Loss: 209.18699964584258\n",
            "Iteration: 240 | Loss: 211.24550460677744\n",
            "Iteration: 260 | Loss: 224.75921771134932\n",
            "Iteration: 280 | Loss: 207.41067721227247\n",
            "Iteration: 300 | Loss: 202.9874547965538\n",
            "Iteration: 320 | Loss: 233.5225806996785\n",
            "Iteration: 340 | Loss: 209.41894810505434\n",
            "Iteration: 360 | Loss: 227.86193173406267\n",
            "Iteration: 380 | Loss: 220.0275230523279\n",
            "Iteration: 400 | Loss: 209.93813706106957\n",
            "Iteration: 420 | Loss: 224.69843019249313\n",
            "Iteration: 440 | Loss: 207.14690567298172\n",
            "Iteration: 460 | Loss: 222.13044724748374\n",
            "Iteration: 480 | Loss: 206.75958921219885\n",
            "Iteration: 500 | Loss: 224.10527985394174\n",
            "Iteration: 520 | Loss: 212.29554595340238\n",
            "Iteration: 540 | Loss: 222.9296852741817\n",
            "Iteration: 560 | Loss: 211.17164394816842\n",
            "Iteration: 580 | Loss: 221.71447794927437\n",
            "Iteration: 600 | Loss: 219.52250985351424\n",
            "Iteration: 620 | Loss: 212.20466048180376\n",
            "Iteration: 640 | Loss: 222.5975165742218\n",
            "Iteration: 660 | Loss: 207.1530791563107\n",
            "Iteration: 680 | Loss: 224.41752826545314\n",
            "Iteration: 700 | Loss: 212.5541881669893\n",
            "Iteration: 720 | Loss: 207.96840920838758\n",
            "Iteration: 740 | Loss: 202.79349339026604\n",
            "Iteration: 760 | Loss: 224.18299606836823\n",
            "Iteration: 780 | Loss: 223.15410816749386\n",
            "Iteration: 800 | Loss: 227.64698698681198\n",
            "Iteration: 820 | Loss: 213.35496804814935\n",
            "Iteration: 840 | Loss: 223.82125080471178\n",
            "Iteration: 860 | Loss: 211.88036740995827\n",
            "Iteration: 880 | Loss: 203.0702406461759\n",
            "Iteration: 900 | Loss: 224.43132254807512\n",
            "Iteration: 920 | Loss: 223.35580653357326\n",
            "Iteration: 940 | Loss: 227.815553049764\n",
            "Iteration: 960 | Loss: 219.9270821314675\n",
            "Iteration: 980 | Loss: 212.53249397223584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5deYnKJ_QC2",
        "outputId": "08d084e3-62e1-4170-e129-494051d5526e"
      },
      "source": [
        "acc = model.evaluate(X_test, Y_test)\n",
        "print (\"SVM is {:.3f}% accurate.\".format(acc * 100))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM is 83.916% accurate.\n"
          ]
        }
      ]
    }
  ]
}